thanos:
  fullnameOverride: thanos
  queryFrontend:
    enabled: true
    extraFlags:
    - --query-range.split-interval=12h
    - --query-frontend.log-queries-longer-than=10s
    - --query-frontend.compress-responses
    - |-
      --query-range.response-cache-config="config":
        "max_size": "500MB"
        "max_size_items": 0
        "validity": 0s
      "type": "in-memory"
  query:
    stores:
      - dnssrv+_grpc._tcp.kube-prometheus-thanos-ruler
## FIXME: Thanos query in Harbor cluster is temporary exposed via (unsecure) NodePort, see https://github.com/SovereignCloudStack/k8s-observability/issues/32
##   Thanos query should be exposed via ingres and then the store address below should be used.
## - dnssrv+_http-harbor-cluster._tcp.thanos-query-envoy
## Uncomment this if you want to observe Harbor cluster via node port
#      - 213.131.230.86:31050
## FIXME: Thanos query in Harbor cluster is temporary exposed via (unsecure) NodePort, see https://github.com/SovereignCloudStack/k8s-observability/issues/32
##   Thanos query in Harbor cluster should be exposed via ingress and then the section below should be used.
#    sidecars:
#      - name: envoy-sidecar
#        image: 'envoyproxy/envoy:v1.21.6'
#        args:
#        - '-c'
#        - /config/envoy.yaml
#        ports:
#        - name: egress-http-1
#          containerPort: 10000
#          protocol: TCP
#        volumeMounts:
#        - name: envoy-config
#          mountPath: /config
#        - name: envoy-certs
#          mountPath: /certs
#    extraVolumes:
#      - name: envoy-config
#        configMap:
#          name: thanos-query-envoy-config
#          defaultMode: 420
#          optional: false
#      - name: envoy-certs
#        secret:
#          secretName: query-observer.dnation.cloud
#          defaultMode: 420
#          optional: false
    replicaCount: 2
    extraFlags:
    - --query.auto-downsampling
    - --query.lookback-delta=1m
#    - --web.external-prefix=thanos # Uncomment if you want run thanos query on different path than /, e.g. /thanos
    replicaLabel: [agent_replica, sidecar_replica, prometheus_replica]
## Uncomment if you want to expose Thanos Query UI via HTTPS endpoint `monitoring.scs.community/thanos`
## TLS is defined via grafana ingress and for oauth see oauth/README.md
#    ingress:
#      enabled: true
#      annotations:
#        nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
#        nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
#      ingressClassName: nginx
#      hostname: monitoring.scs.community
#      path: /thanos
  receive:
    ## Enable thanos receiver if you want to deploy MVP0 version
    enabled: false
    replicaCount: 1
    replicaLabel: receive_replica
    service:
      type: NodePort
      nodePorts:
        remote: 30291
    tsdbRetention: 1d

## FIXME: Thanos query in Harbor cluster is temporary exposed via (unsecure) NodePort, see https://github.com/SovereignCloudStack/k8s-observability/issues/32
##   Thanos query in Harbor cluster should be exposed via ingress and then the section below should be used.
#thanosQueryEnvoySidecar:
#  enabled: true
#  config:
#  - listenPort: 10000
#    name: harbor-cluster
#    queryPort: 443
#    queryUrl: query-harbor.dnation.cloud
#    tls:
#      certificate_chain: /certs/tls.crt
#      private_key: /certs/tls.key
#      trusted_ca: /certs/ca.crt
#  service:
#    labels:
#      name: thanos-query-envoy
#    name: thanos-query-envoy
#    selector:
#      app.kubernetes.io/component: query
#      app.kubernetes.io/name: thanos

grafanaDatasourcesAsConfigMap:
  cluster-metrics:
  - name: thanos
    isDefault: true
    type: prometheus
    access: proxy
    url: http://thanos-query-frontend:9090
#    url: http://thanos-query-frontend:9090/thanos # Replace if thanos query runs on different path than /
## Uncomment if you want to use graphite datasource for zuul
#  zuul-metrics:
#    - name: graphite
#      type: graphite
#      access: proxy
#      url: http://graphite:8080
#      jsonData:
#        graphiteVersion: "1.1"
  cluster-logs: null
loki:
  enabled: false
promtail:
  enabled: false

kube-prometheus-stack:
  grafana:
    service:
      type: NodePort
      nodePort: 30000
## Grafana admin password override (defaults to `pass`)
#    adminPassword: "replaceme"
## Uncomment if you want to expose UI (Grafana) via HTTPS endpoint `monitoring.scs.community`
## see and apply let's encrypt issuer defined in `scs/issuer.yaml`
#    ingress:
#      enabled: true
#      annotations:
#        cert-manager.io/issuer: "letsencrypt-issuer"
#      ingressClassName: nginx
#      hosts:
#      - monitoring.scs.community
#      tls:
#      - secretName: monitoring-grafana-tls
#        hosts:
#        - monitoring.scs.community
## Uncomment if you want to allow anonymous access to the UI (Grafana)
#    env:
#      GF_AUTH_ANONYMOUS_ENABLED: true
## Uncomment if you want to apply a custom SCS branding
#    extraConfigmapMounts:
#      - name: scs-logo
#        mountPath: /usr/share/grafana/public/img/scs_logo.svg
#        subPath: scs_logo.svg
#        configMap: scs-logo
#      - name: dnation-home
#        mountPath: /usr/share/grafana/public/dashboards/home.json
#        subPath: home.json
#        configMap: dnation-home
#    extraSecretMounts:
#      - name: scs-brand
#        mountPath: /etc/secrets
#        secretName: scs-brand
#        defaultMode: 0755
  kubeScheduler:
    serviceMonitor:
      # TODO fix this:
      insecureSkipVerify: true
  kubeControllerManager:
    serviceMonitor:
      # TODO fix this:
      insecureSkipVerify: true
  prometheus:
    prometheusSpec:
      replicas: 2
      replicaExternalLabelName: sidecar_replica
      externalLabels:
        cluster: observer-cluster
  defaultRules:
    labels:
      prometheus_rule: '2'
    disabled:
      PrometheusNotConnectedToAlertmanagers: true
  additionalPrometheusRulesMap:
    dnation-kubernetes-monitoring-rules:
      additionalLabels:
        prometheus_rule: '2'
      groups:
      - name: k8s.rules
        rules:
## Uncomment if you want to deploy MVP0 version
#        - alert: KubernetesMonitoringClusterDown
#          expr: 'kaas unless on(cluster) up'
#          labels:
#            alertgroup: Cluster
#            severity: critical
#          annotations:
#            message: 'Cluster {{ $labels.cluster }} is down.'
        - alert: BlackboxProbeFailed
          expr: 'probe_success == 0'
          for: 5m
          labels:
            severity: critical
          annotations:
            message: 'Blackbox probe on target: {{ $labels.target }} failed'
        - alert: BlackboxSlowProbe
          expr: 'avg_over_time(probe_duration_seconds[1m]) > 5'
          for: 5m
          labels:
            severity: warning
          annotations:
            message: 'Blackbox probe on target: {{ $labels.target }} took more than 5s to complete, probe time = {{ $value }}'
        - alert: BlackboxSslCertificateWillExpireSoon
          expr: 'round((probe_ssl_earliest_cert_expiry - time()) / 86400, 0.1) < 30'
          for: 5m
          labels:
            severity: warning
          annotations:
            message: 'SSL certificate expires in {{ $value }} days'
  nameOverride: kube-prometheus
  alertmanager:
    service:
      type: NodePort
      nodePort: 30001
## Uncomment if you want to expose Alertmanager UI via HTTPS endpoint `monitoring.scs.community/alertmanager`
## TLS is defined via grafana ingress and for oauth see oauth/README.md
#    ingress:
#      enabled: true
#      annotations:
#        nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
#        nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
#      ingressClassName: nginx
#      hosts:
#      - monitoring.scs.community
#      paths:
#      - /alertmanager
## Uncomment if you want to forward alerts into matrix chat
## See also docs in matrix-alertmanager/README.md
#    alertmanagerSpec:
##      routePrefix: /alertmanager # Uncomment if you want run alertmanager on different path than /, e.g. /alertmanager
#      externalUrl: http://localhost:30000
#    config:
#      route:
#        receiver: 'matrix-notifications'
#        group_by: ['alertname', 'job', 'severity']
#        repeat_interval: 24h
#        routes:
#        - receiver: 'null'
#          match:
#            alertname: Watchdog
#      receivers:
#      - name: 'null'
#      - name: 'matrix-notifications'
#        webhook_configs:
#        - url: "http://matrix-alertmanager-receiver:3000/alerts/alert-room"
  thanosRuler:
    enabled: true
    service:
      additionalPorts:
      - name: grpc
        port: 10901
        protocol: TCP
        targetPort: 10901
    thanosRulerSpec:
      evaluationInterval: "1m"
      ruleSelector:
        matchLabels:
          prometheus_rule: '2'
      queryEndpoints:
      - dnssrv+_http._tcp.thanos-query
## Replace if thanos query runs on different path than /
#      - http://thanos-query:9090/thanos
      alertmanagersUrl:
      - http://kube-prometheus-alertmanager:9093
## Replace if alertmanager runs on different path than /
#      - http://kube-prometheus-alertmanager:9093/alertmanager
